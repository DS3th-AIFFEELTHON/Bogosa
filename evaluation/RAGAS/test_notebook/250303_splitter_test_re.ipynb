{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EQuTntQ_9Vp-"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import re\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain_community.vectorstores import Milvus\n",
        "from pymilvus import Collection\n",
        "from uuid import uuid4\n",
        "\n",
        "from datetime import datetime\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableParallel\n",
        "\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_teddynote.retrievers import KiwiBM25Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QCEyBM7c9VqB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def get_pdf_paths(directory):\n",
        "    pdf_paths = []\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.jsonl'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                pdf_paths.append(full_path)\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "# 사용 예시\n",
        "directory_path = \"./parsed_pdf/result_v10/text\"\n",
        "result = get_pdf_paths(directory_path)\n",
        "\n",
        "for resul in result:\n",
        "    resul.replace('\\\\', '/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./parsed_pdf/result_v10/text\\\\KISWeekly제1090호(20240628)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1091호(20240705)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1092호(20240712)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1093호(20240719)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1094호(20240726)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1095호(20240802)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1096호(20240809)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1097호(20240816)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1098호(20240823)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1099호(20240830)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1100호(20240906)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1101호(20240913)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1103호(20240927)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1104호(20241004)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1105호(20241011)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1106호(20241018)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1107호(20241025)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1108호(20241101)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1109호(20241108)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1110호(20241115)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1111호(20241122)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1112호(20241129)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1113호(20241206)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1114호(20241213)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1115호(20241220)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1116호(20241227)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1117호(20250103)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1118호(20250110)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1119호(20250117)_docs_text.jsonl',\n",
              " './parsed_pdf/result_v10/text\\\\KISWeekly제1120호(20250124)_docs_text.jsonl']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vpb0go-e9VqD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain.schema import Document\n",
        "import re\n",
        "a = ''\n",
        "document = []\n",
        "filepath = result[-3]\n",
        "documents = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        \n",
        "        # 각 줄을 JSON으로 파싱\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Document 객체 생성 및 리스트에 추가\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        documents.append(doc)\n",
        "document.append(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NpQmQJYJ9VqE"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "def save_docs_to_jsonl(documents, file_path):\n",
        "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
        "        for doc in documents:\n",
        "            writer.write(doc.dict())\n",
        "\n",
        "URI = 'http://127.0.0.1:19530/'\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FYhCV0QT9VqF"
      },
      "outputs": [],
      "source": [
        "document = [j for i in document for j in i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvB3WugM9VqF",
        "outputId": "4cc77b7c-fc58-4f7c-8c5a-c76e5f433041"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(document)\n",
        "model_name = ['Salesforce/SFR-Embedding-2_R', 'Alibaba-NLP/gte-Qwen2-7B-instruct',\n",
        "              'BAAI/bge-multilingual-gemma2', 'intfloat/e5-mistral-7b-instruct',\n",
        "              'jinaai/jina-embeddings-v3', 'McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised',\n",
        "              'intfloat/multilingual-e5-large-instruct', 'intfloat/multilingual-e5-large',\n",
        "              'BAAI/bge-m3', 'jhgan/ko-sroberta-multitask', 'upskyy/bge-m3-korean',\n",
        "              'nlpai-lab/KoE5']\n",
        "\n",
        "for i in range(len(model_name)):\n",
        "    embeddings = HuggingFaceEndpointEmbeddings(\n",
        "        model=model_name[i],\n",
        "        task='feature-extraction',\n",
        "\n",
        "    )\n",
        "    vectorstore_text = Milvus(\n",
        "        embedding_function=embeddings,\n",
        "        connection_args={'uri':URI},\n",
        "        index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "        collection_name=model_name[i].replace('/', '_') + '_1000_100'\n",
        "    )\n",
        "\n",
        "    save_docs_to_jsonl(splitted_docs, os.path.join('./semantic_test/', model_name[i].replace('/', '_')+'_1000_100'+'.jsonl'))\n",
        "\n",
        "    uuids = [str(uuid4()) for _ in range(len(splitted_docs))]\n",
        "\n",
        "    vectorstore_text.add_documents(\n",
        "        documents=splitted_docs,\n",
        "        ids = uuids\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_name=['openai_large_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2f81e783f57140c690f9f5d7f464e028",
            "e3f7bbcb476c447791b938429d52edc7",
            "571027d3aef64531a9b55f2f4f3bde68",
            "bf4bcf255f414fa9a05373a6c6e6897b"
          ]
        },
        "id": "0ME_n9l49VqK",
        "outputId": "5774a968-d6d3-49b3-f914-bfb4a33c86eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8759e7825f344528b62572527df1c812",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ba6b5aed999482ca6485b946424aacf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "412ffa07bb3b4a208d6eb74ee5d6cdd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef11fb32bdea410e86af8b88c737f3ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "327d57b0a3e4463b82ffcf3ff04cf502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9005e2bf70504016b8530fe57fdc095e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04ba4159eba94ecdb35ad28776d0b76f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3456a547c64249df9ff12b255456162f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "DataTypeNotSupportException",
          "evalue": "<DataTypeNotSupportException: (code=1, message=Field dtype must be of DataType)>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mDataTypeNotSupportException\u001b[0m               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[55], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m save_docs_to_jsonl(splitted_docs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./semantic_test/\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name[i]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_1000_100\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     54\u001b[0m uuids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(splitted_docs))]\n\u001b[1;32m---> 56\u001b[0m \u001b[43mvectorstore_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplitted_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muuids\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m result_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# vectorstore_text = Milvus(\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# embedding_function=embeddings,\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# connection_args={'uri':URI},\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# collection_name=i\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:286\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    285\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m )\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
            "File \u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\milvus.py:581\u001b[0m, in \u001b[0;36mMilvus.add_texts\u001b[1;34m(self, texts, metadatas, timeout, batch_size, ids, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout:\n\u001b[0;32m    580\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# Dict to hold all insert columns\u001b[39;00m\n\u001b[0;32m    584\u001b[0m insert_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_field: texts,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_field: embeddings,\n\u001b[0;32m    587\u001b[0m }\n",
            "File \u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\milvus.py:300\u001b[0m, in \u001b[0;36mMilvus._init\u001b[1;34m(self, embeddings, metadatas, partition_names, replica_number, timeout)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init\u001b[39m(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    293\u001b[0m     embeddings: Optional[\u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m     timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_fields()\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_index()\n",
            "File \u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\milvus.py:375\u001b[0m, in \u001b[0;36mMilvus._create_collection\u001b[1;34m(self, embeddings, metadatas)\u001b[0m\n\u001b[0;32m    364\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    365\u001b[0m         FieldSchema(\n\u001b[0;32m    366\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary_field,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m         )\n\u001b[0;32m    372\u001b[0m     )\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create the vector field, supports binary or float vectors\u001b[39;00m\n\u001b[0;32m    374\u001b[0m fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 375\u001b[0m     \u001b[43mFieldSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dtype_bydata\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m )\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Create the schema for the collection\u001b[39;00m\n\u001b[0;32m    379\u001b[0m schema \u001b[38;5;241m=\u001b[39m CollectionSchema(\n\u001b[0;32m    380\u001b[0m     fields,\n\u001b[0;32m    381\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_description,\n\u001b[0;32m    382\u001b[0m     partition_key_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partition_key_field,\n\u001b[0;32m    383\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\pymilvus\\orm\\schema.py:383\u001b[0m, in \u001b[0;36mFieldSchema.__init__\u001b[1;34m(self, name, dtype, description, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataTypeNotSupportException(message\u001b[38;5;241m=\u001b[39mExceptionsMessage\u001b[38;5;241m.\u001b[39mFieldDtype) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m DataType\u001b[38;5;241m.\u001b[39mUNKNOWN:\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataTypeNotSupportException(message\u001b[38;5;241m=\u001b[39mExceptionsMessage\u001b[38;5;241m.\u001b[39mFieldDtype)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_description \u001b[38;5;241m=\u001b[39m description\n",
            "\u001b[1;31mDataTypeNotSupportException\u001b[0m: <DataTypeNotSupportException: (code=1, message=Field dtype must be of DataType)>"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    AnswerCorrectness,\n",
        "    # answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "import json\n",
        "\n",
        "def convert_to_list(example):\n",
        "    if isinstance(example[\"contexts\"], list):  # 이미 리스트이면 그대로 반환\n",
        "        contexts = example[\"contexts\"]\n",
        "    else:\n",
        "        try:\n",
        "            contexts = json.loads(example[\"contexts\"])  # JSON 문자열이면 변환\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON Decode Error: {example['contexts']} - {e}\")\n",
        "            contexts = []  # 예외 발생 시 빈 리스트로 대체\n",
        "    return {\"contexts\": contexts}\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(document)\n",
        "model_name = [\n",
        "    # 'intfloat/multilingual-e5-large',\n",
        "    # 'BAAI/bge-m3',\n",
        "    # 'jhgan/ko-sroberta-multitask',\n",
        "    # 'upskyy/bge-m3-korean',\n",
        "    # 'nlpai-lab/KoE5',\n",
        "    # 'voyageai/voyage-multilingual-2',\n",
        "    'intfloat/multilingual-e5-base',\n",
        "    'intfloat/multilingual-e5-small',\n",
        "    'cateto/longformer-ko-sroberta-multitask-23040'\n",
        "]\n",
        "\n",
        "for i in range(len(model_name)):\n",
        "    embeddings = HuggingFaceEndpointEmbeddings(\n",
        "        model=model_name[i],\n",
        "        # model_kargs={'device':'cpu'},\n",
        "        # encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "    vectorstore_text = Milvus(\n",
        "        embedding_function=embeddings,\n",
        "        connection_args={'uri':URI},\n",
        "        index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "        collection_name=model_name[i].replace('.', '_').replace('-', '_').replace('/', '_') + '_1000_100'\n",
        "    )\n",
        "\n",
        "    save_docs_to_jsonl(splitted_docs, os.path.join('./semantic_test/', model_name[i].replace('/', '_')+'_1000_100'+'.jsonl'))\n",
        "\n",
        "    uuids = [str(uuid4()) for _ in range(len(splitted_docs))]\n",
        "\n",
        "    vectorstore_text.add_documents(\n",
        "        documents=splitted_docs,\n",
        "        ids = uuids\n",
        "    )\n",
        "\n",
        "    result_ = []\n",
        "    # vectorstore_text = Milvus(\n",
        "    # embedding_function=embeddings,\n",
        "    # connection_args={'uri':URI},\n",
        "    # index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    # collection_name=i\n",
        "    # )\n",
        "\n",
        "    add_date_to_inputs = RunnableLambda(lambda inputs: {\n",
        "    **inputs,\n",
        "        \"current_date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    })\n",
        "\n",
        "    splitted_doc = []\n",
        "    filepath = os.path.join('./semantic_test', model_name[i].replace('/', '_') + '_1000_100'+'.jsonl')\n",
        "    # with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    #     for line in file:\n",
        "    #         if line.startswith('\\n('):\n",
        "    #             continue\n",
        "    #         data = json.loads(line)\n",
        "\n",
        "    #         doc = Document(\n",
        "    #             page_content=data['page_content'],\n",
        "    #             metadata=data['metadata']\n",
        "    #         )\n",
        "    #         splitted_doc.append(doc)\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "\n",
        "    milvus_retriever = vectorstore_text.as_retriever(\n",
        "        search_kwargs={'k':10}\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    df = pd.read_csv(\"./qa_ragas_250302.csv\", index_col=False)\n",
        "    df = df[['question', 'ground_truth']]\n",
        "    df.ground_truth = df.apply(lambda x: x.ground_truth.split(\"'\")[1].split(\"'\")[0], axis=1)\n",
        "\n",
        "    test_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    batch_dataset = [question for question in test_dataset[\"question\"]]\n",
        "    context = milvus_retriever.batch(batch_dataset)\n",
        "\n",
        "    if \"contexts\" in test_dataset.column_names:\n",
        "        test_dataset = test_dataset.remove_columns([\"contexts\"]).add_column(\"contexts\", [[j.page_content for j in i] for i in context])\n",
        "    else:\n",
        "        test_dataset = test_dataset.add_column(\"contexts\", [[j.page_content for j in i] for i in context])\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\n",
        "    '''You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Answer simply.\n",
        "    Answer in Korean.\n",
        "\n",
        "    #Question:\n",
        "    {question}\n",
        "    #Context:\n",
        "    {context}\n",
        "\n",
        "    #Answer:'''\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        RunnableParallel(\n",
        "            context=milvus_retriever,\n",
        "            question=RunnablePassthrough()\n",
        "        )\n",
        "        # | add_date_to_inputs\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    answer = chain.batch(batch_dataset)\n",
        "    answer\n",
        "\n",
        "    if \"answer\" in test_dataset.column_names:\n",
        "        test_dataset = test_dataset.remove_columns([\"answer\"]).add_column(\"answer\", answer)\n",
        "    else:\n",
        "        test_dataset = test_dataset.add_column(\"answer\", answer)\n",
        "\n",
        "    test_dataset = test_dataset.map(convert_to_list)\n",
        "\n",
        "    for _ in range(3):\n",
        "        result = evaluate(\n",
        "            dataset=test_dataset,\n",
        "            metrics=[\n",
        "                context_recall,\n",
        "                context_precision,\n",
        "                answer_similarity,\n",
        "                AnswerCorrectness(weights=[0.75, 0.25]),\n",
        "                # answer_correctness\n",
        "            ],\n",
        "        )\n",
        "        result_.append(result)\n",
        "\n",
        "    with open(filepath.replace('test', 'test/t10').replace('.jsonl', '.json'), \"w\", encoding='utf-8') as json_file:\n",
        "        json.dump(result_, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    AnswerCorrectness,\n",
        "    # answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "import json\n",
        "\n",
        "def convert_to_list(example):\n",
        "    if isinstance(example[\"contexts\"], list):  # 이미 리스트이면 그대로 반환\n",
        "        contexts = example[\"contexts\"]\n",
        "    else:\n",
        "        try:\n",
        "            contexts = json.loads(example[\"contexts\"])  # JSON 문자열이면 변환\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON Decode Error: {example['contexts']} - {e}\")\n",
        "            contexts = []  # 예외 발생 시 빈 리스트로 대체\n",
        "    return {\"contexts\": contexts}\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(document)\n",
        "model_name = [\n",
        "    # 'intfloat/multilingual-e5-large',\n",
        "    # 'BAAI/bge-m3',\n",
        "    # 'jhgan/ko-sroberta-multitask',\n",
        "    # 'upskyy/bge-m3-korean',\n",
        "    # 'nlpai-lab/KoE5',\n",
        "    # 'voyageai/voyage-multilingual-2',\n",
        "    'intfloat/multilingual-e5-base',\n",
        "    'intfloat/multilingual-e5-small',\n",
        "    'cateto/longformer-ko-sroberta-multitask-23040'\n",
        "]\n",
        "\n",
        "for i in range(len(model_name)):\n",
        "    embeddings=UpstageEmbedding\n",
        "    vectorstore_text = Milvus(\n",
        "        embedding_function=embeddings,\n",
        "        connection_args={'uri':URI},\n",
        "        index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "        collection_name=model_name[i].replace('.', '_').replace('-', '_').replace('/', '_') + '_1000_100'\n",
        "    )\n",
        "\n",
        "    save_docs_to_jsonl(splitted_docs, os.path.join('./semantic_test/', model_name[i].replace('/', '_')+'_1000_100'+'.jsonl'))\n",
        "\n",
        "    uuids = [str(uuid4()) for _ in range(len(splitted_docs))]\n",
        "\n",
        "    vectorstore_text.add_documents(\n",
        "        documents=splitted_docs,\n",
        "        ids = uuids\n",
        "    )\n",
        "\n",
        "    result_ = []\n",
        "    # vectorstore_text = Milvus(\n",
        "    # embedding_function=embeddings,\n",
        "    # connection_args={'uri':URI},\n",
        "    # index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    # collection_name=i\n",
        "    # )\n",
        "\n",
        "    add_date_to_inputs = RunnableLambda(lambda inputs: {\n",
        "    **inputs,\n",
        "        \"current_date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    })\n",
        "\n",
        "    splitted_doc = []\n",
        "    filepath = os.path.join('./semantic_test', model_name[i].replace('/', '_') + '_1000_100'+'.jsonl')\n",
        "    # with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    #     for line in file:\n",
        "    #         if line.startswith('\\n('):\n",
        "    #             continue\n",
        "    #         data = json.loads(line)\n",
        "\n",
        "    #         doc = Document(\n",
        "    #             page_content=data['page_content'],\n",
        "    #             metadata=data['metadata']\n",
        "    #         )\n",
        "    #         splitted_doc.append(doc)\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "\n",
        "    milvus_retriever = vectorstore_text.as_retriever(\n",
        "        search_kwargs={'k':10}\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    df = pd.read_csv(\"./qa_ragas_250302.csv\", index_col=False)\n",
        "    df = df[['question', 'ground_truth']]\n",
        "    df.ground_truth = df.apply(lambda x: x.ground_truth.split(\"'\")[1].split(\"'\")[0], axis=1)\n",
        "\n",
        "    test_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    batch_dataset = [question for question in test_dataset[\"question\"]]\n",
        "    context = milvus_retriever.batch(batch_dataset)\n",
        "\n",
        "    if \"contexts\" in test_dataset.column_names:\n",
        "        test_dataset = test_dataset.remove_columns([\"contexts\"]).add_column(\"contexts\", [[j.page_content for j in i] for i in context])\n",
        "    else:\n",
        "        test_dataset = test_dataset.add_column(\"contexts\", [[j.page_content for j in i] for i in context])\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\n",
        "    '''You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Answer simply.\n",
        "    Answer in Korean.\n",
        "\n",
        "    #Question:\n",
        "    {question}\n",
        "    #Context:\n",
        "    {context}\n",
        "\n",
        "    #Answer:'''\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        RunnableParallel(\n",
        "            context=milvus_retriever,\n",
        "            question=RunnablePassthrough()\n",
        "        )\n",
        "        # | add_date_to_inputs\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    answer = chain.batch(batch_dataset)\n",
        "    answer\n",
        "\n",
        "    if \"answer\" in test_dataset.column_names:\n",
        "        test_dataset = test_dataset.remove_columns([\"answer\"]).add_column(\"answer\", answer)\n",
        "    else:\n",
        "        test_dataset = test_dataset.add_column(\"answer\", answer)\n",
        "\n",
        "    test_dataset = test_dataset.map(convert_to_list)\n",
        "\n",
        "    for _ in range(3):\n",
        "        result = evaluate(\n",
        "            dataset=test_dataset,\n",
        "            metrics=[\n",
        "                context_recall,\n",
        "                context_precision,\n",
        "                answer_similarity,\n",
        "                AnswerCorrectness(weights=[0.75, 0.25]),\n",
        "                # answer_correctness\n",
        "            ],\n",
        "        )\n",
        "        result_.append(result)\n",
        "\n",
        "    with open(filepath.replace('test', 'test/t10').replace('.jsonl', '.json'), \"w\", encoding='utf-8') as json_file:\n",
        "        json.dump(result_, json_file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
